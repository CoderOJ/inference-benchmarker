apiVersion: apps/v1
kind: Deployment
metadata:
  name: text-generation-inference
  labels:
    app: text-generation-inference
spec:
  replicas: 1
  selector:
    matchLabels:
      app: text-generation-inference
  strategy:
    type: Recreate
  template:
    metadata:
      name: text-generation-inference
      labels:
        app: text-generation-inference
    spec:
      containers:
        - name: text-generation-inference
          image: ghcr.io/huggingface/text-generation-inference:latest
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 80
              protocol: TCP
          env:
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: HF_TOKEN
          args:
            - "--model-id"
            - "meta-llama/Llama-3.1-8B-Instruct"
            - "--max-concurrent-requests"
            - "512"
          volumeMounts:
            - name: shm
              mountPath: /dev/shm
          resources:
            limits:
              "nvidia.com/gpu": "1"
              cpu: "3"
              memory: "10Gi"
      restartPolicy: Always
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 1Gi
---
apiVersion: v1
kind: Service
metadata:
  name: text-generation-inference
spec:
  selector:
    app: text-generation-inference
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 80
  type: ClusterIP
---
apiVersion: batch/v1
kind: Job
metadata:
  name: text-generation-inference-benchmark
  labels:
    app: text-generation-inference-benchmark
spec:
  template:
    metadata:
      name: text-generation-inference-benchmark
      labels:
        app: text-generation-inference-benchmark
    spec:
      initContainers:
        - name: wait-for-text-generation-inference
          image: busybox
          command:
            - sh
            - -c
            - |
              until nslookup text-generation-inference.default.svc.cluster.local; do
                echo "Waiting for text-generation-inference service..."
                sleep 1
              done
              exit 0
      containers:
        - name: text-generation-inference-benchmark
          image: ghcr.io/huggingface/text-generation-inference-benchmark:latest
          imagePullPolicy: Always
          ports:
            - containerPort: 80
              protocol: TCP
          env:
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: HF_TOKEN
            - name: RUST_LOG
              value: "text_generation_inference_benchmark=info"
          command:
            - "text-generation-inference-benchmark"
            - "--tokenizer-name"
            - "meta-llama/Llama-3.1-8B-Instruct"
            - "--max-vus"
            - "800"
            - "--url"
            - "http://text-generation-inference.default.svc.cluster.local:8080"
            - "--duration"
            - "5m"
            - "--warmup"
            - "30s"
            - "--benchmark-kind"
            - "rate"
            - "--prompt-options"
            - "num_tokens=200,max_tokens=220,min_tokens=180,variance=10"
            - "--decode-options"
            - "num_tokens=200,max_tokens=220,min_tokens=180,variance=10"
            - "--no-console"
            - "--rates"
            - "0.8"
            - "--rates"
            - "1.6"
            - "--rates"
            - "2.4"
            - "--rates"
            - "3.2"
            - "--rates"
            - "4.0"
            - "--rates"
            - "4.8"
            - "--rates"
            - "5.6"
            - "--rates"
            - "6.4"
            - "--rates"
            - "7.2"
            - "--rates"
            - "8.0"
            - "--rates"
            - "8.8"
            - "--rates"
            - "9.6"
            - "--rates"
            - "10.4"
            - "--rates"
            - "11.2"
            - "--rates"
            - "12.0"
            - "--rates"
            - "12.8"
            - "--rates"
            - "13.6"
            - "--rates"
            - "14.4"
            - "--rates"
            - "15.2"
            - "--rates"
            - "16.0"
            - "--rates"
            - "16.8"
            - "--rates"
            - "17.6"
            - "--rates"
            - "18.4"
            - "--rates"
            - "19.2"
            - "--rates"
            - "20.0"
            - "--rates"
            - "20.8"
            - "--rates"
            - "21.6"
            - "--rates"
            - "22.4"
            - "--rates"
            - "23.2"
            - "--rates"
            - "24.0"
          volumeMounts:
            - name: results
              mountPath: /opt/text-generation-inference-benchmark/results
          lifecycle:
            preStop:
              exec:
                command: ["sleep", "9999999"]
      restartPolicy: OnFailure
      volumes:
        - name: results
          emptyDir: { }
